{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ed9c54",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "> Author: Michael Gao\n",
    "\n",
    "> Instruction on submission:\n",
    ">  * Complete all the tasks listed. Answer any questions posed in the tasks.\n",
    ">  * When you are completed with this assignment, please clear all the cells and run it from beginning to end to ensure that I can run it. \n",
    ">  * Afterwards, submit the ipynb file AND an html version of the notebook to Sakai\n",
    ">  * To obtain the HTML version, run the notebook in its entirety and go to File > Download as > html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ee1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ebf5b",
   "metadata": {},
   "source": [
    "## Task List\n",
    "***\n",
    "\n",
    "[Task 1: Perceptrons from scratch](#task1)\n",
    "\n",
    "[Task 2: Perceptron Learning Algorithm](#task2)\n",
    "\n",
    "[Task 3: Unlearnability of the XOR function](#task3)\n",
    "\n",
    "[Task 4: Number of parameters in the network](#task4)\n",
    "\n",
    "[Task 5: If one XOR is unlearnable ...](#task5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3830c5b",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbe9fa",
   "metadata": {},
   "source": [
    "In class, we discussed some of the history of neural networks and the algorithms and tools needed to fit them. In particular, the gradient descent and backpropagation algorithms -- combined with modern computational capabilities -- have allowed neural networks to become one of the most powerful tools in machine learning. \n",
    "\n",
    "Our goal for this homework/tutorial is to trace through the history of the development of precursors to neural networks, motivate their use, and then see how modern tools (Pytorch) allow for a convenient way to fit these networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cec40a",
   "metadata": {},
   "source": [
    "## Perceptron from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c4e733",
   "metadata": {},
   "source": [
    "Recall that the perceptron learns the following decision rule:\n",
    "![](./assets/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674f712",
   "metadata": {},
   "source": [
    "In class, we stated that perceptrons can only learn *linearly separable* decision rules. Let's see an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb8068",
   "metadata": {},
   "source": [
    "We will begin by implementing this from scratch using numpy. Let's begin by creating the linearly separable example from class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ca2fc",
   "metadata": {},
   "source": [
    "### AND function \n",
    "We will first show that that the perceptron can learn the AND function. You can refer to the slides from class if you forget what the AND function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AND_data = [[0, 0], [0, 1], [1, 1], [1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1c5b4",
   "metadata": {},
   "source": [
    "# Task 1 <a name=\"task1\"></a>\n",
    "Write a function which iterates through each row in the numpy array and returns the corresponding labels as a list.\n",
    "Store the labels in a variable called `AND_labels`. \n",
    "`AND_labels` should be a length 4 list with the correct labels for the AND function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and(dat):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    dat: List[List[int]]\n",
    "        A list of lists containing 1s and 0s in \n",
    "        the first and second element places\n",
    "    Returns\n",
    "    -------\n",
    "    l: List[int]\n",
    "        A list with the same length of dat with 1s and 0s that \n",
    "        correspond to the \"answers\" of the AND function \n",
    "    \"\"\"\n",
    "    # Your Code Here\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75488f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "AND_labels = # Run the function and store the result here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2af240",
   "metadata": {},
   "source": [
    "Now let's create a perceptron that will accomplish this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.w0 = 0\n",
    "        self.w1 = 0\n",
    "        self.w2 = 0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Implement the decision rule here\n",
    "        Recall that you can access the weights of this object\n",
    "        using the self.w0 syntax\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        x: List[int]\n",
    "            A list of length 2 which has 0s and/or 1s \n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        A 1 or 0 depending on the x input\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70937b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "percep = Perceptron() # Here we instantiate your perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3b9ff",
   "metadata": {},
   "source": [
    "#### Show the current predictions of your perceptron\n",
    "that is, call the predict method using `percep.predict()` on each possible combination of 1 and 0 to see what the current prediction is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1790ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f00e38",
   "metadata": {},
   "source": [
    "# Task 2: Learning algorithm <a name=\"task2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710346e3",
   "metadata": {},
   "source": [
    "as you've seen in the previous task, our perceptron algorithm did not correctly predict the answers of the AND task. We now need to implement an algorithm to update the weights of our perceptron model so that it correctly predicts the outcome.\n",
    "\n",
    "From class, we saw that this can be accomplished using an update rule. Refer to this rule and implement the loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa23d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_perceptron_weights(perceptron, data, labels, learning_rate=0.05):\n",
    "    \"\"\"\n",
    "    Runs 1 iteration of the weight update algorithm for the simple\n",
    "    perceptron. This is an impure function, and will directly modify the \n",
    "    perceptron object that it is called on.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    perceptron: Perceptron\n",
    "        An instantiated perceptron object\n",
    "    data: List[List[int]]\n",
    "        The data being used to train the model\n",
    "    labels: List[int]\n",
    "        The labels for the associated data\n",
    "    learning_rate: float\n",
    "        The learning rate to multiply the update by. \\eta in the notes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (updates the perceptron object)\n",
    "    \"\"\"\n",
    "    for index, row in enumerate(data):\n",
    "        predicted_label = \n",
    "        actual_label = label[index]\n",
    "        \n",
    "        # Update the weights of the perceptron\n",
    "        perceptron.w0 = \n",
    "        perceptron.w1 = \n",
    "        perceptron.w2 =\n",
    "    pass # don't return anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c4ea2",
   "metadata": {},
   "source": [
    "Now write a function to check whether your current perceptron actually predicts the correct result! It should return `True` if the perceptron has learned the AND function correctly and `False` if otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_perceptron(perceptron, data, labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9459e79",
   "metadata": {},
   "source": [
    "Now, run the update algorithm with a learning rate of `0.05` (the default) and verify the perceptron at the end of each run through the data (1 epoch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c682842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62739379",
   "metadata": {},
   "source": [
    "How many epochs did it take to converge to the correct solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f8537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "679b2990",
   "metadata": {},
   "source": [
    "Print out the final weights of your perceptron algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1c0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de21b42",
   "metadata": {},
   "source": [
    "# Task 3 <a name=\"task3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec09015",
   "metadata": {},
   "source": [
    "Now, using the same logic as above, show that, even after a large number (>1000) of iterations, the same perceptron cannot learn the XOR function. You are allowed to reuse logic that you created above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR IMPLEMENTATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938597d",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptrons "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e487e57",
   "metadata": {},
   "source": [
    "As discussed in class, multi-layer perceptrons *are* able to represent the XOR function. However, it was initially difficult to train multi-layer perceptrons. We now know that by using gradient descent and backpropagation, we can train multi-layer neural networks (a generalization of the perceptron). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e1e42",
   "metadata": {},
   "source": [
    "First, we will define the following network: <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60004bf",
   "metadata": {},
   "source": [
    "![](./assets/multi-layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544b36f",
   "metadata": {},
   "source": [
    "# Task 4: <a name=\"task4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6e3f5",
   "metadata": {},
   "source": [
    "How many learnable parameters does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b0f18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9895cc75",
   "metadata": {},
   "source": [
    "Given an input size p, a hidden layer with k nodes, and all linear layers, what is an expression for the number of parameters of a similar network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46fb1a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e59721a",
   "metadata": {},
   "source": [
    "# Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681da141",
   "metadata": {},
   "source": [
    "As you have seen, it can be very tedious to keep track of all of the computations and gradients needed to fit these algorithms. Especially as the number of inputs and the number of hidden layers increases, it becomes completely infeasible to keep track of the gradients and weights of each unit. This is where modern software tools makes life easy. Pytorch (along with other alternatives such as Tensorflow / JAX / etc.) are all used to fit neural networks and abstract away the record keeping and gradient computation. These libraries all implement something known as *autodiff*, or automatic differentiation. Instead of us having to compute the derivatives ourselves, these libraries will keep track of everything. \n",
    "\n",
    "In this course, we will use Pytorch, though the advantages of one library or another are up for debate. This will serve as a quick introduction to pytorch, though I would highly recommend looking at the official [pytorch documentation](https://pytorch.org/docs/stable/index.html) for more information. \n",
    "\n",
    "Let's take a look at a high-level how Pytorch works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5263a",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04976aa",
   "metadata": {},
   "source": [
    "`pytorch.Tensors` are the fundamental data structure that are used in tensors. They are analogous to Numpy `ndarray`s. This means that you can perform all of the regular operations on tensors that you might expect (addition / subtraction / concatenation / transposition / matrix multiplication, etc.) Let's construct a tensor and examine it. For more detailed information on how Tensors operate, please consult [the documentation](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fb5e0",
   "metadata": {},
   "source": [
    "#### Constructing Tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3c0f5",
   "metadata": {},
   "source": [
    "you can either construct a tensor by simply passing it a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.Tensor([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca37f94",
   "metadata": {},
   "source": [
    "or from a corresponding numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ab647",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2 = torch.from_numpy(np.array([0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584caa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29dd9e4",
   "metadata": {},
   "source": [
    "though there are many other methods you can use to create torch tensors, these are some of the most common. The other most common will be when loading data in, which we will discuss later "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9bae2",
   "metadata": {},
   "source": [
    "#### Common operations (analogous to numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0008fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 + tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa09975",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor1 + 6) * (tensor2 + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f204ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((tensor1, tensor2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8f2b5",
   "metadata": {},
   "source": [
    "#### Tracking of gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffe319",
   "metadata": {},
   "source": [
    "A key property of a tensor is the `.requires_grad` attribute. Essentially, this is what allows a tensor to keep track of all of the operations that have been performed on it as well as its gradients! Let's see an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = torch.Tensor([2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1dd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set this to true\n",
    "example_tensor.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor.grad_fn\n",
    "# Note that this currently returns nothing, \n",
    "# since no operations have been performed on it yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = example_tensor ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849942a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_tensor.grad_fn\n",
    "# Now you'll see that there is a grad_fn, \n",
    "# which is the derivative of the square function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e8a1c",
   "metadata": {},
   "source": [
    "Essentially, every built-in operation that is computed on Tensors keeps track of its own gradient and operations, so that when it comes time to update using gradient descent, it can refer to the list of computations that was performed on it and \"roll them up\" to calculate an update. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd723246",
   "metadata": {},
   "source": [
    "Now that we've seen this, let's take a look at 2 more important concepts that are necessary for us to finally train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59b69a",
   "metadata": {},
   "source": [
    "## Loss Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991d38a",
   "metadata": {},
   "source": [
    "loss functions are functions will tell us how far away from a desired result our current result is. They are a critical component in training neural networks since they determine exactly what the model will optimize for. There are many loss functions that one could choose for each problem, and construction of loss functions is a very deep topic. For our purposes, we will often choose the canonical choice for problems (e.g. cross entropy for binary classification problems, squared error loss for regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad77ba",
   "metadata": {},
   "source": [
    "pytorch's loss functions have a method called `.backward()` which will essentially calculate all of the gradients necessary to perform a gradient step in gradient descent when used in conjunction with the neural network functions. Here is an example of a loss function in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss() \n",
    "# Rather than just calling the function, we need to instantiate \n",
    "# it so that it can be used over and over (and keep track of gradients) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bd1ec",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a172bc",
   "metadata": {},
   "source": [
    "The optimizer is the method by which we update weights in a neural network! In class, we discussed the gradient descent algorithm, which as a pytorch optimizer is the `torch.Optim.SGD`. In reality, this stands for **Stochastic Gradient Descent**, which is a slight modification of the original gradient descent algorithm that allows for more efficient exploration of the loss surface. We will not concern ourselves with the details at this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ee327",
   "metadata": {},
   "source": [
    "`optimizer = torch.optim.SGD(params= , lr= )`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ac013",
   "metadata": {},
   "source": [
    "We need to provide 2 things to this optimizer -- the parameters that we interested in updating, and the learning rate! The learning rate, using the notation that we discussed in class, is $\\eta$ in the $\\Delta w = \\eta \\cdot \\frac{\\partial E}{\\partial w}$ term. We usually choose this number to be small (for example 0.05) to avoid overshooting our minimum loss. The choice of loss functions is a current area of research that can often dramatically affect the performance of our models, especially as they become more complex. Many methods have tried to be robust to the choice of learning rate or to adapt the learning rate over time. We will discuss these in future classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eef33b",
   "metadata": {},
   "source": [
    "# A step back "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b2376",
   "metadata": {},
   "source": [
    "We have now looked at several aspects of pytorch and how they relate to what you have learned in class about fitting neural networks. However, it is important to take a step back and reflect on what pytorch is actually doing and how it can be a powerful tool for implementing neural network (and other!) models. \n",
    "\n",
    "When we constructed a multi-layer perceptron, we noted that all of a sudden there were many parameters to keep track of and a lot of tedious bookkeeping. Pytorch certainly makes this easy for us. However, it can do so much more! Note that pytorch and other so-called autograd software allows us to implement other algorithms that deal with derivatives and create custom loss functions, etc., etc. Although we will mainly use Pytorch for neural networks, just keep in mind that it can be used for much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb729c80",
   "metadata": {},
   "source": [
    "## Creating our model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6429e4",
   "metadata": {},
   "source": [
    "Recall that our [model](#model) contains 2 input nodes, a hidden layer of 2 input nodes, and an output node that contains our prediction of interest. How do we create such a model in pytorch and have it keep track of everything? We can use the `torch.nn`Â module to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd63bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's use the Torch version of sigmoid function. \n",
    "# Remember, we often need to instantiate functions in pytorch so they can do record keeping for us\n",
    "\n",
    "sig_fn = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Don't worry if you don't understand everything going on here. The key highlights\n",
    "    are as follows:\n",
    "        the init block tells us what happens when we create a new object of this class\n",
    "        here, we use the nn.Module superclass, so that's why we call super(MultiPerceptron).\n",
    "        This is where all the pytorch magic is held\n",
    "        \n",
    "        Next, we define each layer using layers that we find in torch.nn\n",
    "        Again, we are just instantiating them for now.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MultiPerceptron, self).__init__()\n",
    "        self.input = nn.Linear(in_features=2, out_features=2)\n",
    "        self.output = nn.Linear(in_features=2, out_features=1)\n",
    "    \n",
    "    # Next, we define what happens as data enters the model. How does it flow in the \"forward\"\n",
    "    # direction?\n",
    "    def forward(self, x):\n",
    "        # X, presumably, is the input data, so a torch tensor of size 2\n",
    "        x = self.input(x) # first, it goes through the input layer runs b_0 + w_1x_1 + w_2x_2\n",
    "        x = sig_fn(x) # Run it through the sigmoid function \n",
    "        x = self.output(x) # Now take those outputs and run them through another linear layer\n",
    "        x = sig_fn(x) # Finally, run the output through a sigmoid again\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43441d7c",
   "metadata": {},
   "source": [
    "Make sure you understand each step that is happening above! This is the basics of how to define almost any network in Pytorch. All we are doing is defining exactly how data moves through our model. Now, we need to train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006469c",
   "metadata": {},
   "source": [
    "Let's go through a standard recipe for training this model from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d031d3",
   "metadata": {},
   "source": [
    "#### 1. Get your input data ready\n",
    "For this, we're going to create some data with labels that represents the XOR function, which if you recall cannot be learned by the original perceptron algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfeb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.array([[0, 0], [0, 1], [1, 0], [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x[0] ^ x[1] for x in dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67bffb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dat)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501359f",
   "metadata": {},
   "source": [
    "Now convert them into pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b52b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = torch.Tensor(dat)\n",
    "labels = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbfbde",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMLP = MultiPerceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663cfa8f",
   "metadata": {},
   "source": [
    "#### 3. Instantiate the loss function, optimizer and any other functions that you need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb803b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_fn = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b50630",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(myMLP.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff43edb",
   "metadata": {},
   "source": [
    "#### 4. Create the training loop!\n",
    "> The loop will consist of the following:\n",
    ">  * Feed the data in and run it through the network\n",
    ">  * Compute the loss of the expected target, y, versus the output model(x)\n",
    ">  * call .backward() to accumulate all of the gradients\n",
    ">  * tell the optimizer to perform 1 step of gradient descent\n",
    ">  * Zero out the gradients (or else the optimizer will keep adding to the previous iterations)\n",
    ">  * repeat (until convergence, or several other stopping criterion that we will discuss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9743e66",
   "metadata": {},
   "source": [
    "Model parameter values before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74982bda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name, param in myMLP.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf72f0",
   "metadata": {},
   "source": [
    "Model Predictions before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a68557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model, input_data, labels):\n",
    "    \"\"\"\n",
    "    Returns a pandas dataframe that contains the input data, label, and prediction for\n",
    "    easy comparison\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model: torch.nn.Module\n",
    "        Contains the model of interest\n",
    "    input_data: torch.Tensor\n",
    "        The input dataset to compare results on\n",
    "    labels: torch.Tensor\n",
    "        The correct answer to be compared against\n",
    "    \"\"\"\n",
    "    input_data_ = []\n",
    "    labels_ = []\n",
    "    predictions_ = []\n",
    "    \n",
    "    for index, row in enumerate(input_data):\n",
    "        input_data_.append(list(row))\n",
    "        labels_.append(labels[index].item())\n",
    "        # Compute model predictions\n",
    "        predictions_.append(model(row).item())\n",
    "    \n",
    "    return pd.DataFrame({'input': input_data_, 'Target': labels_, 'Prediction': predictions_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ff206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(myMLP, dat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000 # each epoch is 1 run through the data.\n",
    "for epoch in range(num_epochs):\n",
    "    for index, x in enumerate(dat):\n",
    "        # Run it through the network\n",
    "        output = myMLP(x)\n",
    "        # Compute the loss\n",
    "        loss_ = loss_fn(output, labels[index].view(1)) # Add the view(1) to get the right sizes to line up... this is a common error in pytorch\n",
    "        # Accumulate gradients\n",
    "        loss_.backward()\n",
    "        # Tell the optimizer to step\n",
    "        optim.step()\n",
    "        # Zero out any gradients (since they accumulate)\n",
    "        optim.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabdf41f",
   "metadata": {},
   "source": [
    "model outputs after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e67c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in myMLP.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca0faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(myMLP, dat, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db52c40",
   "metadata": {},
   "source": [
    "# Task 5 <a name=\"task5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a17faf",
   "metadata": {},
   "source": [
    "Now, your turn! For this task, you will attempt to create a network that mimics the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e80637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor3(x, y, z):\n",
    "    # (x XOR y) XOR z\n",
    "    # Finish this function!\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90327a",
   "metadata": {},
   "source": [
    "This takes in 3 inputs and computes an XOR function on the first 2 and then an XOR function on the result of this operation with the third input. After implementing this function, you create the following dataset (without trivially copying):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d28f2",
   "metadata": {},
   "source": [
    "![](./assets/xor3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bc850",
   "metadata": {},
   "source": [
    "To see that this is not linearly separable, please refer to this crude drawing that I made:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f24716",
   "metadata": {},
   "source": [
    "![](./assets/Xor3_box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf7756",
   "metadata": {},
   "source": [
    "Note that no 2-dimensional plane can separate the red and blue dots!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b14dd",
   "metadata": {},
   "source": [
    "Your goal is to create a network and follow the steps above that can mimic this function. Feel free to get creative and add more layers, etc. and verify that your network can adequately learn this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cfc08",
   "metadata": {},
   "source": [
    "# Resources and important topics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8618a",
   "metadata": {},
   "source": [
    "#### Pytorch Dataloaders "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce88ba",
   "metadata": {},
   "source": [
    "One aspect of pytorch that we did not touch upon on this homework was the concept of Dataloaders and Datasets. As you get more complicated data, you may need to load the data into your model in batches. This has several advantages, both computational as well as algorithmic. One key point is that in our current implementation, the weights only get updated once per epoch, which can be quite cumbersome if the epochs are large. Therefore it may make sense to batch the data and update for each batch. This is what pytorch Dataloaders provide methods for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c674dc",
   "metadata": {},
   "source": [
    "To read more about dataloaders and their usage, please refer to [this tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) and [this link](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) for a complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d744eaf",
   "metadata": {},
   "source": [
    "#### Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd57dd",
   "metadata": {},
   "source": [
    "Backpropagation (and gradient descent) are incredibly important topics in modern-day machine learning. One invaluable resource that I have found is [this video](https://youtu.be/IHZwWFHWa-w) by 3Blue1Brown, which goes over in detail with amazing visualizations how gradient descent works in the context of neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
